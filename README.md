<h1 style="text-align: center;"> Modeling Survival in model-based Reinforcement Learning</h1>
<ul style="text-align: justify;">
<h2><strong>Abstract</strong></h2>
<p style="text-align: justify;">The standard framework in the context of reinforcement learning is to find an optimal policy to select actions to maximize a cumulative reward for a task. A common trend in developing many reinforcement learning methods is that the agent must interact with the environment thousands or even millions of times to learn an optimal policy. This has limited the application of reinforcement learning to simulations, video games, and a few simple physical systems. Reducing the number of faults from which an agent can learn to behave correctly is essential to reduce the risk of physical implementations and decrease the computation costs. Several efforts have been made to mitigate this problem seeking to reduce the risk of failure to make algorithms applicable for real-world everyday tasks. 
With the aim to contribute to this area, this research project explores the notion of survival in Reinforcement Learning by investigating the effectiveness of safety maps in order to improve the action sample efficiency in robotic applications. We discuss why the agent is not always interested in maximizing expected rewards; sometimes, the goal that the agent is trying to accomplish is to survive. Next, we propose a set of model-based reinforcement learning methods to train an agent to avoid dangerous states through a safety map model built with temporal credit assignment. Finally, we investigate a collection of different variants for the presented method along with a comparison between proposed and current methods.
</p>
