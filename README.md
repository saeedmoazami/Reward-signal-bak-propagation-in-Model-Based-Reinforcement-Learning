# Modeling Survival in model-based Reinforcement Learning.

## Abstract

Although Recent model-free reinforcement learning algorithms have shown to be capable of mastering complicated decision-making tasks, the sample complexity of these methods has remained a hurdle to utilize them in many real-world applications. In this regard, model-based reinforcement learning proposes some remedies. Yet, inherently, model-based methods are more computationally expensive and susceptible to sub-optimality. One reason is that model-generated data are always less accurate than real data, and this often leads to inaccurate transition and reward function models. With the aim to mitigate this problem, we explain how we can train a substitute to the reward function approximator while avoiding unnecessary data to create a more efficient and accurate model. To that end, we present the notion of survival by discussing cases in which the agentâ€™s goal is to survive and its analogy to maximizing the expected rewards. Next, we propose a model-based reinforcement learning method to train an agent to avoid dangerous states through a safety map model built upon temporal credit assignment in the vicinity of terminal states. Finally, we investigate the presented algorithm, along with a comparison between the proposed and current methods.
